{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2587,"status":"ok","timestamp":1660019630161,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"AOPCunu64A6g"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\kusw-04\\Anaconda3\\envs\\deep\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import DataLoader\n","import torch\n","import torch.nn as nn\n","import re"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1660025057512,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"MwqWM-tc4Fsp"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"jeZObPIr4A6l"},"source":["# 모델제작"]},{"cell_type":"markdown","metadata":{"id":"3yJ7CX1F4A6o"},"source":["# 데이터 준비"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":23,"status":"error","timestamp":1660044583090,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"GRxjgt1_4A6p","outputId":"53abe03e-3023-4c45-f87b-17168aab53f6"},"outputs":[],"source":["with open('./chars-4996', encoding='utf-8-sig') as f:\n","    content = f.read()\n","    keys = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"] + list(content)\n","vocab = dict()\n","for i, key in enumerate(keys): vocab[key] = i"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1660044583087,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"YwOKEm-x4A6p"},"outputs":[],"source":["with open('./namuwikitext_20200302.dev', 'r', encoding='utf-8') as f:\n","    content = f.readlines()"]},{"cell_type":"markdown","metadata":{"id":"GQN0uCeV4A6r"},"source":["# string, label 제작\n","1. string 변형\n","2. 라벨제작\n","3. Padding"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1660025613488,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"FR96LvSm4A6r"},"outputs":[],"source":["add_prob = 0.5\n","del_prob = 0.15\n","\n","seq_len = 128\n","\n","def labeling(data):\n","    string_data = torch.Tensor()\n","    label_data = torch.Tensor()\n","\n","    i = 0\n","    while i < len(data):\n","        # 문장의 끝이 아니고, 다음 단어가 띄어쓰기일 경우\n","        is_space = True if (i < len(data)-1) and (data[i+1] == vocab[\" \"]) else False\n","\n","        if is_space: # '다음'문자가 띄어쓰기면, 띄어쓰기를 제거하거나\n","            state = 2 if torch.rand(1) < del_prob else 0\n","        else: # '다음' 문자가 띄어쓰기가 아니라면, 띄어쓰기를 추가\n","            state = 1 if data[i] != vocab[\" \"] and torch.rand(1) < add_prob else 0\n","        \n","        # string\n","        # state 0: data[i]\n","        # state 1: data[i] + \" \"\n","        # state 2: data[i] // '다음' 띄어쓰기 제거  // 이건 state0과 같이 동작한 뒤, 나중에 index jump로 해결\n","        string_data = torch.cat((string_data, torch.tensor([data[i], vocab[\" \"]])), dim=0) if state == 1 else torch.cat((string_data, torch.tensor([data[i]])), dim=0)\n","        \n","        # label\n","        # state 0: 0\n","        # state 1: 2// 띄어쓰기를 추가한 경우 이므로, 제거하라는 라벨 붙이기\n","        # state 2: 1// 다음 띄어쓰기를 제거했으므로, 추가하라는 의미 부여\n","        if state == 0:\n","            label_data = torch.cat((label_data, torch.tensor([0])), dim=0)\n","        elif state == 1:\n","            label_data = torch.cat((label_data, torch.tensor([0, 2])), dim=0)\n","        else:\n","            label_data = torch.cat((label_data, torch.tensor([1])), dim=0)\n","        # One hot vector로 표현\n","        \"\"\"\n","        if state == 0:\n","            label_data = torch.cat((label_data, torch.tensor([[0, 0, 0]])), dim=0)\n","        elif state == 1:\n","            label_data = torch.cat((label_data, torch.tensor([[0, 0, 0]])), dim=0)\n","            label_data = torch.cat((label_data, torch.tensor([[0, 0, 1]])), dim=0)\n","        else:\n","            label_data = torch.cat((label_data, torch.tensor([[0, 1, 0]])), dim=0)\n","        \"\"\"\n","        # 띄어쓰기를 삭제한 경우, 다음 data 건너 뛰기\n","        i += 2 if state == 2 else 1\n","\n","    # 문자열 Padding\n","    string_data = torch.cat((torch.tensor([vocab[\"<s>\"]]), string_data, torch.tensor([vocab[\"</s>\"]])), dim=0)\n","    string_data = torch.cat((string_data, torch.tensor([vocab['<pad>']] * (seq_len - len(string_data)))), dim=0) if len(string_data) < seq_len else string_data[:seq_len]\n","\n","    # 라벨 Padding\n","    label_data = torch.cat((torch.tensor([0]), label_data, torch.tensor([0])), dim=0)\n","    label_data = torch.cat((label_data, torch.tensor([-1] * (seq_len - len(label_data)))), dim=0) if len(label_data) < seq_len else label_data[:seq_len]\n","    # 라벨 One-hot padding\n","    # label_data = torch.cat((torch.tensor([[0, 0, 0]]), label_data, torch.tensor([[0, 0, 0]])), dim=0)\n","    # label_data = torch.cat((label_data, torch.tensor([[0, 0, 0]] * (seq_len - len(label_data)))), dim=0) if len(label_data) < seq_len else label_data[:seq_len]\n","    \n","    return string_data, label_data\n"]},{"cell_type":"markdown","metadata":{"id":"YPrg-10k4A6t"},"source":["# Dataset 구성"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56284,"status":"ok","timestamp":1660025039193,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"dd83x66d4A6t","outputId":"d7986828-721e-4f7a-bb99-32928396f3d9"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 5000/5000 [02:28<00:00, 33.75it/s] \n"]}],"source":["from tqdm import tqdm\n","\n","string_ds = torch.Tensor()\n","label_ds = torch.Tensor()\n","\n","# 이렇게 선언해놓으면 병렬처리 어려움\n","# 한번에 다 받은다음에 나중에 쪼개는게 훨 빠름\n","for line in tqdm(content[:5000]):\n","    data = torch.Tensor()\n","    line = ' '.join(line.strip().split())   # 띄어쓰기 2개 이상 있는 문장 제거\n","    for char in line:\n","        try:\n","            data = torch.cat((data, torch.tensor([vocab[char]])), dim=0)\n","        except:\n","            data = torch.cat((data,torch.tensor([vocab['<unk>']])), dim=0)\n","    if len(data) > 0:\n","        string_data, label_data = labeling(data)\n","        string_ds = torch.cat((string_ds, string_data), dim=0)\n","        label_ds = torch.cat((label_ds, label_data), dim=0)\n","    \n","string_ds = string_ds.view(len(string_ds) // seq_len, seq_len)\n","label_ds = label_ds.view(len(label_ds) // seq_len, seq_len)"]},{"cell_type":"markdown","metadata":{"id":"STLvs2w64A6u"},"source":["# Batch 만들기"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1660025050921,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"kKuTAQV34A6u","outputId":"47e074b0-b05f-4511-b271-1c2bbaed286c"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","string_ds = string_ds.type(torch.long)\n","label_ds = label_ds.type(torch.long)\n","string_dl = DataLoader(string_ds, batch_size=64, shuffle=True, num_workers=2, pin_memory=False)\n","label_dl = DataLoader(label_ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=False)"]},{"cell_type":"code","execution_count":77,"metadata":{"executionInfo":{"elapsed":289,"status":"ok","timestamp":1660038432485,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"z02r8TOu4A6n"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class SpacingModel(nn.Module):\n","    def __init__(\n","        self, \n","        vocab_size, \n","        hidden_size, \n","        num_classes = 3, \n","        conv_activation=\"relu\", \n","        dense_activation=\"relu\",\n","        kernel_and_filter_sizes = [\n","            (2, 8),\n","            (3, 8),\n","            (4, 8),\n","            (5, 8)\n","        ],\n","        dropout_rate = 0.3\n","        ):\n","        super(SpacingModel, self).__init__()\n","\n","        # 5000개(vocab_size)의 단어를 각각 48차원(hidden_size)으로 Embedding 진행\n","        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n","        \n","        in_channels = hidden_size\n","        # 2, 3, 4, 5 증가하는 Kernel size는 1d Conv에서, Ngram의 역할 수행\n","        # conv1d (Batch, Channel, Seq_len) (in:Channel-in, out: Channel-out)\n","        \n","        self.convs = list()\n","        for kernel_size, filter_size in kernel_and_filter_sizes:\n","            layer = nn.Sequential(\n","                nn.Conv1d(in_channels=in_channels, out_channels=filter_size, kernel_size=kernel_size, padding=kernel_size//2, bias=False),\n","                nn.ReLU(inplace=True)\n","            )\n","            self.convs.append(layer)\n","\n","        # 한번에 몇개씩 Pooling할지 설정\n","        self.pools = []\n","        for _, filter_size in kernel_and_filter_sizes:\n","            self.pools.append(nn.MaxPool1d(filter_size))\n","\n","        self.dropout1 = nn.Dropout(p=dropout_rate)\n","        self.linear1 = nn.Sequential(\n","            # @@@pooling한 filter의 합\n","            nn.Linear(len(kernel_and_filter_sizes), hidden_size),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.dropout2 = nn.Dropout(p=dropout_rate)\n","        self.linear2 = nn.Linear(hidden_size, num_classes)    \n","    \n","    def forward(self, x):\n","        \n","        #Tensorflow / embedded: [batch, seq_len, hidden_size]\n","        #Pytorch / embedded: [batch, hidden_size(channel), seq_len(len)]로 바꿔줘야함\n","        \n","        # Embedding 만 딱 마쳤을때는 [batch, seq_len, hidden_size] 그래서 permute로 바꿔줌\n","        # x shape = [batch, seq_len, channel]을 permute시켜서 [batch, channel(hidden_size), seq_len]으로 변경\n","        x = self.embeddings(x).permute(0, 2, 1)\n","        features = []\n","        for conv, pool in zip(self.convs, self.pools):\n","            y = conv(x)\n","            # 현재 y shape = [batch, channel, seq_len]\n","            y = pool(y.permute(0, 2, 1))    # seq_len은 놔두고, channel에 대해서만 pooling\n","            # 현재 y shape = [batch, seq_len, channel]\n","            #print('conv shape:', y.shape)\n","            features.append(y)\n","        \n","        # features shape = [batch, seq_len, #filters의 합]\n","        features = torch.cat(features, dim=-1)\n","        features = self.dropout1(features)\n","\n","        # projected shape = (batch, seq_len, hidden_size)\n","        projected = self.dropout2(self.linear1(features))\n","\n","        # result (batch, seq_len, 3)\n","        # after permute (batch, 3, seq_len) // loss 계산을 위해 [batch, nb_classes, seq_len] 형식으로 변경\n","        result = self.linear2(projected).permute(0, 2, 1)\n","        #print('result shape:', result.shape)\n","        return result"]},{"cell_type":"markdown","metadata":{"id":"vuXnN8jU6XzF"},"source":["# Train part"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["from torch.optim.lr_scheduler import ReduceLROnPlateau"]},{"cell_type":"code","execution_count":91,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"executionInfo":{"elapsed":789,"status":"error","timestamp":1660038435042,"user":{"displayName":"이찬현","userId":"15256857878106465993"},"user_tz":-540},"id":"X9WiWgr66b2B","outputId":"132ff271-44e9-407d-b523-c50b18cd1307"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([2, 2, 2, 2, 2, 2, 2, 2])\n","tensor([2, 2, 2, 2, 2, 2, 2, 2])\n","tensor([2, 2, 2, 2, 2, 2, 2, 2])\n","tensor([2, 2, 2, 2, 2, 2, 2, 2])\n","tensor([2, 2, 2, 2, 2, 2, 2, 2])\n","tensor([2, 2, 2, 2, 2, 2, 2, 2])\n","tensor([2, 2, 2, 2, 2, 0, 0, 2])\n","tensor([2, 0, 2, 2, 2, 2, 2, 2])\n","tensor([0, 2, 2, 2, 2, 2, 0, 0])\n","tensor([2, 2, 2, 0, 2, 0, 0, 2])\n","tensor([0, 2, 2, 0, 0, 0, 2, 2])\n","tensor([0, 2, 2, 0, 0, 2, 0, 0])\n","tensor([0, 0, 0, 2, 0, 0, 2, 0])\n","tensor([0, 0, 0, 0, 0, 0, 2, 0])\n","tensor([2, 2, 2, 0, 0, 2, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 2])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 2, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","[Train] | epoch: 1/30 | batch: 20/62 | loss: 0.8549 | Acc: 18.3505\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","[Train] | epoch: 1/30 | batch: 40/62 | loss: 0.7278 | Acc: 24.5320\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n","Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n","tensor([0, 0, 0, 0, 0, 0, 0, 0])\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22600\\338069409.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[Train] | epoch: {epoch+1}/{epochs} | batch: {index+1}/{len(string_dl)} | loss: {loss.item():.4f} | Acc: {correct / total * 100:.4f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22600\\338069409.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m#print(outputs, targets)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# loss 미분하여 Grad 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# w, b 적용하여 초기화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\kusw-04\\Anaconda3\\envs\\deep\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\kusw-04\\Anaconda3\\envs\\deep\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["vocab_size = 5000\n","hidden_size = 48    # Embedding size\n","kernel_and_filter_sizes = [[3, 8], [5, 16], [7, 16], [9, 16]]\n","model = SpacingModel(\n","    vocab_size=vocab_size,\n","    hidden_size=hidden_size,\n","    num_classes=3,\n","    conv_activation='relu',\n","    dense_activation='relu',\n","    kernel_and_filter_sizes=kernel_and_filter_sizes,\n","    dropout_rate=0.1)\n","model.to(device)\n","\n","#criterion = nn.NLLLoss(ignore_index=-1)\n","#criterion = nn.CrossEntropyLoss()\n","criterion = nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# lr scheduler 도입\n","lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=1)\n","\n","best_acc = 0\n","epochs = 30\n","predicted= list()\n","\n","def train(epoch):\n","    model.train()\n","\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    \n","    for index, (inputs, targets) in enumerate(zip(string_dl, label_dl)):\n","        inputs, targets = inputs.to(device),targets.to(device)\n","        optimizer.zero_grad()   # optimizer에 저장되어있던 gradient 제거\n","\n","        outputs = model(inputs)\n","        # print('-'*32)\n","        # print(outputs[0][0][:8])\n","        # print(outputs[0][1][:8])\n","        # print(outputs[0][2][:8])\n","        # print(targets[0][:8])\n","\n","        #print(outputs, targets)\n","        loss = criterion(outputs, targets)\n","        loss.backward() # loss 미분하여 Grad 계산\n","        optimizer.step() # w, b 적용하여 초기화\n","        \n","        lr_scheduler.step(loss)\n","\n","        #train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        print(predicted[0][:8])\n","        # print(predicted[0][:8])\n","        # print('-'*32)\n","        \n","        total += targets.size(0) * seq_len    # 64 (==batch_size) * seq_len\n","        correct += (predicted == targets).sum().item()\n","        if (index+1) % 20 == 0:\n","            print(f'[Train] | epoch: {epoch+1}/{epochs} | batch: {index+1}/{len(string_dl)} | loss: {loss.item():.4f} | Acc: {correct / total * 100:.4f}')\n","for epoch in range(epochs):\n","    train(epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlusXY44xSNu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jeZObPIr4A6l","3yJ7CX1F4A6o","GQN0uCeV4A6r"],"name":"challenge.ipynb","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.13 ('deep')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"146440546cc7423bcb9e6ab2d642d77136a5d09419d2404b62495c9c2eae0394"}}},"nbformat":4,"nbformat_minor":0}
