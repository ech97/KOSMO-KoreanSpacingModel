{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--char-file'], dest='char_file', nargs=None, const=None, default='chars-4996', type=<class 'str'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--train-file\", type=str, required=True, default=\"./namuwikitext_20200302.dev\")\n",
    "parser.add_argument(\"--dev-file\", type=str, required=True, default=\"./namuwikitext_20200302.dev\")\n",
    "parser.add_argument(\"--training-config\", type=str, required=True, default=\"./config.json\")\n",
    "parser.add_argument(\"--char-file\", type=str, required=True, default=\"chars-4996\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpacingModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        num_classes: int = 3,\n",
    "        conv_activation: str = \"relu\",\n",
    "        dense_activation: str = \"relu\",\n",
    "        conv_kernel_and_filter_sizes: List[Tuple[int, int]] = [\n",
    "            (2, 8),\n",
    "            (3, 8),\n",
    "            (4, 8),\n",
    "            (5, 8),\n",
    "        ],\n",
    "        dropout_rate: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = tf.keras.layers.Embedding(vocab_size, hidden_size)\n",
    "        self.convs = [\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filter_size,\n",
    "                kernel_size,\n",
    "                padding=\"same\",\n",
    "                activation=conv_activation,\n",
    "            )\n",
    "            for kernel_size, filter_size in conv_kernel_and_filter_sizes\n",
    "        ]\n",
    "        self.pools = [\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=filter_size, data_format=\"channels_first\")\n",
    "            for _, filter_size in conv_kernel_and_filter_sizes\n",
    "        ]\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.output_dense1 = tf.keras.layers.Dense(hidden_size, activation=dense_activation)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.output_dense2 = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        \"\"\"\n",
    "        input_tensor: Tokenized Sequences, Shape: (Batch Size, Sequence Length)\n",
    "        \"\"\"\n",
    "\n",
    "        # embeddings: (Batch Size, Sequence Length, Hidden Size)\n",
    "        embeddings = self.embeddings(input_tensor)\n",
    "        # features: (Batch Size, Sequence Length, sum(#filters))\n",
    "        features = self.dropout1(\n",
    "            tf.concat([pool(conv(embeddings)) for conv, pool in zip(self.convs, self.pools)], axis=-1)\n",
    "        )\n",
    "        # projected: (Batch Size, Sequence Length, Hidden Size)\n",
    "        projected = self.dropout2(self.output_dense1(features))\n",
    "        # (Batch Size, Sequence Length, 2)\n",
    "        return self.output_dense2(projected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_example(\n",
    "    vocab_table: tf.lookup.StaticHashTable,\n",
    "    encoding: str = \"UTF-8\",\n",
    "    max_length: int = 256,\n",
    "    delete_prob: float = 0.5,\n",
    "    add_prob: float = 0.15,\n",
    "):\n",
    "    @tf.function\n",
    "    def _inner(tensors: tf.Tensor):\n",
    "        bytes_array = tf.strings.unicode_split(tf.strings.regex_replace(tensors, \" +\", \" \"), encoding)\n",
    "        space_positions = bytes_array == \" \"\n",
    "        sequence_length = tf.shape(space_positions)[0]\n",
    "        \n",
    "        while_condition = lambda i, *_: i < sequence_length\n",
    "\n",
    "        def while_body(i, strings, labels):\n",
    "            # 다음 char가 space가 아니고, 문장 끝이 아닐 때 add_prob의 확률로 space 추가\n",
    "            # 이번 char가 space일 때\n",
    "            is_next_char_space = tf.cond(i < sequence_length - 1, lambda: bytes_array[i + 1] == \" \", lambda: False)\n",
    "\n",
    "            state = tf.cond(\n",
    "                is_next_char_space,\n",
    "                lambda: tf.cond(tf.random.uniform([]) < delete_prob, lambda: 2, lambda: 0),\n",
    "                lambda: tf.cond(bytes_array[i] != \" \" and tf.random.uniform([]) < add_prob, lambda: 1, lambda: 0),\n",
    "            )\n",
    "            # 0: 그대로 진행\n",
    "            # 1: 다음 인덱스에 space 추가\n",
    "            # 2: 다음 space 삭제\n",
    "            strings = tf.cond(\n",
    "                state != 1,\n",
    "                lambda: tf.concat([strings, [bytes_array[i]]], axis=0),\n",
    "                lambda: tf.concat([strings, [bytes_array[i], \" \"]], axis=0),\n",
    "            )\n",
    "            # label 0: 변화 x\n",
    "            # label 1: 다음 인덱스에 space 추가\n",
    "            # label 2: 현재 space 삭제\n",
    "            labels = tf.cond(\n",
    "                state == 0,\n",
    "                lambda: tf.concat([labels, [0]], axis=0),\n",
    "                lambda: tf.cond(\n",
    "                    state == 1,\n",
    "                    lambda: tf.concat([labels, [0, 2]], axis=0),\n",
    "                    lambda: tf.concat([labels, [1]], axis=0),\n",
    "                ),\n",
    "            )\n",
    "            i += tf.cond(state == 2, lambda: 2, lambda: 1)\n",
    "\n",
    "            return (i, strings, labels)\n",
    "\n",
    "        i, strings, labels = tf.while_loop(\n",
    "            while_condition,\n",
    "            while_body,\n",
    "            (\n",
    "                tf.constant(0),\n",
    "                tf.constant([], dtype=tf.string),\n",
    "                tf.constant([], dtype=tf.int32),\n",
    "            ),\n",
    "            shape_invariants=(tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([None])),\n",
    "        )\n",
    "\n",
    "        strings = vocab_table.lookup(tf.concat([[\"<s>\"], strings, [\"</s>\"]], axis=0))\n",
    "        labels = tf.concat([[0], labels, [0]], axis=0)\n",
    "\n",
    "        strings = tf.cond(tf.shape(strings)[0] > max_length, lambda: strings[:max_length], lambda: strings)\n",
    "        labels = tf.cond(tf.shape(labels)[0] > max_length, lambda: labels[:max_length], lambda: labels)\n",
    "\n",
    "        length_to_pad = max_length - tf.shape(strings)[0]\n",
    "        strings = tf.pad(strings, [[0, length_to_pad]])\n",
    "        labels = tf.pad(labels, [[0, length_to_pad]], constant_values=-1)\n",
    "\n",
    "        return (strings, labels)\n",
    "\n",
    "    return _inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python as tp\n",
    "\n",
    "def sparse_categorical_crossentropy_with_ignore(y_true, y_pred, from_logits=False, axis=-1, ignore_id=-1):\n",
    "    positions = tf.where(y_true != ignore_id)\n",
    "\n",
    "    y_true = tf.gather_nd(y_true, positions)\n",
    "    y_pred = tf.gather_nd(y_pred, positions)\n",
    "\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=from_logits, axis=axis)\n",
    "\n",
    "\n",
    "def sparse_categorical_accuracy_with_ignore(y_true, y_pred, ignore_id=-1):\n",
    "    positions = tf.where(y_true != ignore_id)\n",
    "\n",
    "    y_true = tf.gather_nd(y_true, positions)\n",
    "    y_pred = tf.gather_nd(y_pred, positions)\n",
    "\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "class SparseCategoricalCrossentropyWithIgnore(tp.keras.losses.LossFunctionWrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        from_logits=False,\n",
    "        reduction=tf.keras.losses.Reduction.AUTO,\n",
    "        ignore_id=-1,\n",
    "        name=\"sparse_categorical_crossentropy_with_ignore\",\n",
    "    ):\n",
    "        super(SparseCategoricalCrossentropyWithIgnore, self).__init__(\n",
    "            sparse_categorical_crossentropy_with_ignore,\n",
    "            name=name,\n",
    "            reduction=reduction,\n",
    "            ignore_id=ignore_id,\n",
    "            from_logits=from_logits,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config.json', 'r', encoding='utf-8-sig') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./chars-4996') as f:\n",
    "    content = f.read()\n",
    "    keys = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"] + list(set(content))\n",
    "    values = list(range(len(keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_initializer = tf.lookup.KeyValueTensorInitializer(keys, values, key_dtype=tf.string, value_dtype=tf.int32)\n",
    "vocab_table = tf.lookup.StaticHashTable(vocab_initializer, default_value=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.lookup_ops.StaticHashTable object at 0x0000019CF1ABF348>\n"
     ]
    }
   ],
   "source": [
    "print(vocab_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function outer_factory.<locals>.inner_factory.<locals>.tf___inner.<locals>.<lambda> at 0x0000019CF2EF8C18>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = (\n",
    "        tf.data.TextLineDataset(tf.constant(['./namuwikitext_20200302.dev']))\n",
    "        .shuffle(10000)\n",
    "        .map(\n",
    "            string_to_example(vocab_table),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "        )\n",
    "        .batch(config[\"train_batch_size\"])\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "146440546cc7423bcb9e6ab2d642d77136a5d09419d2404b62495c9c2eae0394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
